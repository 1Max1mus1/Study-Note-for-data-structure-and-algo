# 《动手学深度学习》第二章 预备知识 - 学习笔记

> 💡 **本章目标**: 掌握深度学习所需的基础工具和数学知识
>  这一章是工具箱,不需要全部记忆,知道在哪里查阅即可!

------

## 📊 2.1 数据操作 (张量基础)

### 什么是张量 (Tensor)?

张量是深度学习框架中存储和处理数据的主要工具,可以理解为**多维数组**。

```
标量 (0维): 一个数字        如: 5
向量 (1维): 一行数字        如: [1, 2, 3, 4]
矩阵 (2维): 二维表格        如: [[1,2], [3,4]]
张量 (n维): n维数组         如: 三维图像数据
```

### 核心操作

#### 1. 创建张量

```python
import torch

# 创建一维张量(向量)
x = torch.arange(12)  # [0, 1, 2, ..., 11]

# 创建指定形状的张量
x = torch.zeros((2, 3, 4))  # 2×3×4的全0张量

# 创建随机张量
x = torch.randn(3, 4)  # 3×4的标准正态分布随机数
```

#### 2. 张量属性

```python
x = torch.arange(12).reshape(3, 4)

x.shape      # 形状: torch.Size([3, 4])
x.numel()    # 元素总数: 12
x.dtype      # 数据类型: torch.float32
```

#### 3. 基本运算

```python
# 按元素运算
x + y    # 加法
x * y    # 乘法 (注意:这是对应元素相乘,不是矩阵乘法!)
x ** 2   # 平方
torch.exp(x)  # 指数运算

# 矩阵乘法
torch.matmul(A, B)  # 或 A @ B
```

#### 4. 广播机制 (Broadcasting)

当两个张量形状不同时,PyTorch会自动"扩展"较小的张量以匹配较大张量的形状。

```python
a = torch.arange(3).reshape(3, 1)  # [[0], [1], [2]]
b = torch.arange(2).reshape(1, 2)  # [[0, 1]]

# 广播后相加
c = a + b  
# 结果: [[0, 1],
#       [1, 2],
#       [2, 3]]
```

**广播规则**: 从最右边的维度开始比较,维度大小要么相等,要么其中一个为1。

#### 5. 索引和切片

```python
X = torch.arange(12).reshape(3, 4)

X[0]       # 第一行
X[:, 1]    # 第二列  
X[0:2, :]  # 前两行
X[1, 2]    # 第二行第三列的元素

# 修改元素
X[1, 2] = 9
```

#### 6. 节省内存

```python
# 不推荐: 会分配新内存
Y = X + Y  

# 推荐: 原地操作,节省内存
Y[:] = X + Y
# 或
Y += X
```

### 🎯 重点提示

- 张量是深度学习的基本数据结构
- 掌握创建、索引、运算这三类操作即可
- 广播机制很强大,但要小心形状不匹配的错误

------

## 🗂️ 2.2 数据预处理

在实际应用中,原始数据通常需要预处理才能用于训练模型。

### 读取数据集

```python
import pandas as pd

# 创建CSV文件
data = pd.DataFrame({
    'NumRooms': [3, 2, 4, 3],
    'Alley': ['Pave', None, None, None],
    'Price': [127500, 106000, 178100, 140000]
})

# 读取数据
data = pd.read_csv('house.csv')
```

### 处理缺失值

#### 方法1: 插值

用均值、中位数等填充缺失值

```python
# 用均值填充数值型缺失值
inputs = data.iloc[:, 0:2]
inputs = inputs.fillna(inputs.mean())
```

#### 方法2: 删除

直接删除包含缺失值的行或列

```python
# 删除含有缺失值的行
data = data.dropna()
```

#### 方法3: 独热编码 (One-Hot Encoding)

将类别变量转换为数值

```python
# 将类别列转换为独热编码
inputs = pd.get_dummies(inputs, dummy_na=True)

# 原始: Alley列 = ['Pave', NaN, NaN]
# 转换后: Alley_Pave = [1, 0, 0]
#         Alley_nan = [0, 1, 1]
```

### 转换为张量

```python
import torch

X = torch.tensor(inputs.values)
y = torch.tensor(data['Price'].values)
```

### 🎯 重点提示

- 真实数据几乎总是"脏"的,需要清洗
- 缺失值处理没有完美方法,要根据具体情况选择
- 类别变量需要转换为数值才能输入模型

------

## 🧮 2.3 线性代数

线性代数是深度学习的数学基础,用于处理表格数据。

### 基本概念

#### 1. 标量 (Scalar)

单个数字,0维张量

```python
x = torch.tensor(3.0)
```

#### 2. 向量 (Vector)

一维数组,1维张量

```python
x = torch.tensor([1, 2, 3, 4])
len(x)  # 长度: 4
```

**向量的几何意义**: 可以看作空间中的一个点或一个箭头(有方向和长度)

#### 3. 矩阵 (Matrix)

二维数组,2维张量

```python
A = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])
A.shape  # (2, 3) - 2行3列
```

**转置**: 行列互换

```python
A.T  # 转置矩阵
```

**对称矩阵**: A = A^T

#### 4. 张量 (Tensor)

n维数组

```python
X = torch.arange(24).reshape(2, 3, 4)  # 3维张量
```

### 重要运算

#### 1. 降维 (Reduction)

```python
A = torch.arange(6).reshape(2, 3)

A.sum()           # 所有元素求和
A.sum(axis=0)     # 沿第0轴(行)求和,得到每列的和
A.sum(axis=1)     # 沿第1轴(列)求和,得到每行的和
A.mean()          # 平均值
```

**保持维度**:

```python
A.sum(axis=1, keepdims=True)  # 保持原来的维度形状
```

#### 2. 点积 (Dot Product)

两个向量的**对应元素相乘再求和**

```python
x = torch.tensor([1., 2., 3.])
y = torch.tensor([4., 5., 6.])

torch.dot(x, y)  # 1*4 + 2*5 + 3*6 = 32
```

**几何意义**: 衡量两个向量的相似度

#### 3. 矩阵-向量乘法

```python
A = torch.tensor([[1., 2.], [3., 4.]])
x = torch.tensor([1., 2.])

torch.mv(A, x)  # 结果: [5., 11.]
```

**计算过程**:

```
[1, 2]   [1]   [1*1 + 2*2]   [5]
[3, 4] · [2] = [3*1 + 4*2] = [11]
```

#### 4. 矩阵-矩阵乘法

```python
A = torch.tensor([[1., 2.], [3., 4.]])
B = torch.tensor([[5., 6.], [7., 8.]])

torch.mm(A, B)  # 矩阵乘法
```

**规则**: (m×n) 矩阵 × (n×p) 矩阵 = (m×p) 矩阵

⚠️ **注意**: `A * B` 是对应元素相乘,不是矩阵乘法!

#### 5. 范数 (Norm)

衡量向量的"大小"或"长度"

**L2范数** (欧几里得距离):

```python
x = torch.tensor([3., 4.])
torch.norm(x)  # √(3² + 4²) = 5
```

**L1范数** (曼哈顿距离):

```python
torch.abs(x).sum()  # |3| + |4| = 7
```

**矩阵的Frobenius范数**:

```python
torch.norm(A)  # 所有元素平方和的平方根
```

### 🎯 重点提示

- 向量和矩阵是神经网络计算的基础
- 矩阵乘法是最常用的运算,要理解其规则
- 范数用于度量模型参数的大小,在正则化中很重要

------

## 📐 2.4 微积分

微积分是优化算法的理论基础,帮助我们找到最优的模型参数。

### 为什么需要微积分?

在机器学习中,我们要**最小化损失函数**来找到最佳参数:

```
找到参数 θ,使得 Loss(θ) 最小
```

微积分告诉我们如何沿着"下坡"的方向调整参数!

### 核心概念

#### 1. 导数 (Derivative)

**定义**: 函数在某点的变化率

```
f'(x) = lim[h→0] (f(x+h) - f(x)) / h
```

**几何意义**: 切线的斜率

**常见导数**:

```
(x^n)' = n·x^(n-1)
(e^x)' = e^x
(ln x)' = 1/x
(sin x)' = cos x
```

#### 2. 偏导数 (Partial Derivative)

多元函数对某一个变量求导,其他变量视为常数

```
函数: f(x, y) = x²y + y³

∂f/∂x = 2xy    (对x求偏导,y视为常数)
∂f/∂y = x² + 3y²  (对y求偏导,x视为常数)
```

#### 3. 梯度 (Gradient)

**定义**: 所有偏导数组成的向量

```
∇f = [∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ]
```

**几何意义**:

- 指向函数增长最快的方向
- 梯度的反方向是函数下降最快的方向

**例子**:

```python
f(x, y) = x² + 2y²

∇f = [2x, 4y]

# 在点(1, 2)处
∇f(1,2) = [2, 8]  # 函数在此点沿[2,8]方向增长最快
```

#### 4. 链式法则 (Chain Rule)

**核心思想**: 复合函数的导数等于外层函数导数×内层函数导数

```
设 y = f(u), u = g(x)
则 dy/dx = (dy/du) · (du/dx)
```

**例子**:

```
y = (x² + 1)³

设 u = x² + 1, 则 y = u³

dy/dx = (dy/du) · (du/dx) = 3u² · 2x = 6x(x² + 1)²
```

**深度学习中的应用**: 反向传播算法就是链式法则的应用!

### 在深度学习中的应用

#### 梯度下降

```
重复以下步骤直到收敛:
  θ = θ - η · ∇Loss(θ)
  
其中:
  θ: 参数
  η: 学习率
  ∇Loss(θ): 损失函数的梯度
```

### 🎯 重点提示

- 导数告诉我们函数如何变化
- 梯度是多元函数的导数,指向增长最快的方向
- 链式法则是反向传播的数学基础
- 不需要手动求导!深度学习框架会自动计算

------

## 🤖 2.5 自动微分 (Automatic Differentiation)

### 为什么需要自动微分?

手动求导复杂模型的梯度:

- ❌ 容易出错
- ❌ 耗时费力
- ❌ 无法处理复杂网络

**解决方案**: 让计算机自动计算梯度!

### 基本用法

#### 1. 简单例子

```python
import torch

# 创建需要梯度的张量
x = torch.arange(4.0, requires_grad=True)

# 定义函数
y = 2 * torch.dot(x, x)  # y = 2x₁² + 2x₂² + 2x₃² + 2x₄²

# 自动计算梯度
y.backward()  # 反向传播

# 查看梯度
print(x.grad)  # dy/dx = [4x₁, 4x₂, 4x₃, 4x₄]
```

#### 2. 工作原理

PyTorch会构建一个**计算图** (Computational Graph):

```
输入 x → 运算1 → 中间结果 → 运算2 → 输出 y
  ↑                                    |
  |<------- 梯度反向传播 ----------------|
```

**前向传播**: 从输入计算输出
 **反向传播**: 从输出计算梯度

#### 3. 非标量的反向传播

当y不是标量时,需要指定gradient参数:

```python
x = torch.arange(4.0, requires_grad=True)
y = x * x  # y是向量

# 需要指定与y形状相同的gradient
y.backward(torch.ones(len(y)))

print(x.grad)  # [0, 2, 4, 6]
```

#### 4. 分离计算

有时我们想把某些计算从计算图中分离出来:

```python
x = torch.arange(4.0, requires_grad=True)
y = x * x
u = y.detach()  # u与y值相同,但不需要梯度
z = u * x

z.sum().backward()
print(x.grad)  # 只包含z对x的直接梯度
```

#### 5. Python控制流的梯度

即使有条件语句、循环,自动微分也能正确计算梯度:

```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c

a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()

print(a.grad)  # 自动处理所有控制流!
```

### 计算图的生命周期

```python
# 第一次反向传播
y.backward()

# 默认情况下,计算图会被释放
# 再次调用会报错:
# y.backward()  # 错误!

# 如果需要多次反向传播:
y.backward(retain_graph=True)  # 保留计算图
```

### 梯度清零

PyTorch默认会**累积**梯度:

```python
x.grad.zero_()  # 清零梯度

# 或者在优化器中:
optimizer.zero_grad()
```

### 🎯 重点提示

- `requires_grad=True` 告诉PyTorch追踪该张量的运算
- `.backward()` 自动计算梯度并存储在 `.grad` 中
- 梯度会累积,记得清零!
- 计算图在反向传播后默认被释放

------

## 🎲 2.6 概率

概率论帮助我们处理不确定性,这在机器学习中无处不在。

### 为什么需要概率?

- 数据往往有噪声
- 模型的预测不是绝对确定的
- 我们需要量化不确定性

### 基本概念

#### 1. 概率的定义

```
P(A) = 事件A发生的可能性 ∈ [0, 1]

P(A) = 0  → 不可能发生
P(A) = 1  → 必然发生
P(A) = 0.5 → 各占一半
```

#### 2. 概率公理

```
1. 非负性: P(A) ≥ 0
2. 归一性: P(样本空间) = 1
3. 可加性: P(A∪B) = P(A) + P(B) (当A,B互斥时)
```

#### 3. 条件概率

**定义**: 在B发生的条件下,A发生的概率

```
P(A|B) = P(A,B) / P(B)

读作: "A给定B的概率"
```

**例子**:

```
P(下雨|乌云密布) = 下雨且乌云密布的概率 / 乌云密布的概率
```

#### 4. 贝叶斯定理 (Bayes' Theorem)

从P(B|A)推导P(A|B)的公式:

```
P(A|B) = P(B|A) · P(A) / P(B)

其中:
  P(A|B): 后验概率 (我们想要的)
  P(B|A): 似然
  P(A): 先验概率
  P(B): 证据
```

**应用**: 垃圾邮件分类、疾病诊断等

#### 5. 独立性

如果A和B独立,则:

```
P(A,B) = P(A) · P(B)
P(A|B) = P(A)
```

### 随机变量

#### 离散随机变量

只能取有限个或可数个值

```python
# 抛硬币
P(正面) = 0.5
P(反面) = 0.5

# 掷骰子  
P(1) = P(2) = ... = P(6) = 1/6
```

#### 连续随机变量

可以取无限个值

**概率密度函数** (PDF):

```
P(a ≤ X ≤ b) = ∫[a到b] p(x)dx
```

### 常见分布

#### 1. 均匀分布

所有结果等可能

#### 2. 正态分布 (高斯分布)

最重要的连续分布!

```
N(μ, σ²)

μ: 均值 (中心位置)
σ²: 方差 (分散程度)
σ: 标准差
```

**标准正态分布**: N(0, 1)

**性质**:

- 钟形曲线,关于均值对称
- 68%的数据在μ±σ范围内
- 95%的数据在μ±2σ范围内

```python
import torch

# 生成正态分布随机数
x = torch.normal(mean=0, std=1, size=(1000,))
```

### 期望和方差

#### 期望 (Expected Value)

平均值,随机变量的"重心"

```
E[X] = Σ x·P(x)  (离散)
E[X] = ∫ x·p(x)dx  (连续)
```

#### 方差 (Variance)

衡量随机变量的分散程度

```
Var(X) = E[(X - E[X])²]
```

**标准差**: σ = √Var(X)

### 🎯 重点提示

- 概率用于量化不确定性
- 条件概率在深度学习中无处不在(如分类问题)
- 正态分布是最常用的分布
- 不需要深入的概率论知识,理解基本概念即可

------

## 📚 2.7 查阅文档

学会查文档比记住所有API更重要!

### PyTorch文档查阅

#### 1. 查看模块中的所有函数

```python
import torch

# 查看torch模块的所有内容
print(dir(torch))

# 查看torch.nn模块
import torch.nn as nn
print(dir(nn))
```

#### 2. 查看函数/类的用法

```python
# 查看详细文档
help(torch.ones)

# 在Jupyter中更方便:
torch.ones?   # 显示文档
torch.ones??  # 显示文档+源代码
```

#### 3. 在线文档

官方文档: https://pytorch.org/docs/

- 搜索功能强大
- 有示例代码
- 有教程和指南

### 常用资源

- **PyTorch官方教程**: https://pytorch.org/tutorials/
- **PyTorch论坛**: https://discuss.pytorch.org/
- **Stack Overflow**: 搜索具体问题
- **GitHub Issues**: 查看已知问题

### 🎯 重点提示

- 不要试图记住所有函数!
- 学会快速查找和理解文档
- 善用搜索引擎
- 看官方示例代码是最快的学习方式

------

## 🎯 第二章总结

### 核心要点回顾

1. **数据操作 (2.1)**
   - 张量是深度学习的基本数据结构
   - 掌握创建、索引、运算三类基本操作
   - 注意广播机制和内存管理
2. **数据预处理 (2.2)**
   - 真实数据需要清洗(处理缺失值、格式转换)
   - 类别变量要转换为数值(独热编码)
   - 最后转换为张量才能输入模型
3. **线性代数 (2.3)**
   - 标量、向量、矩阵、张量的概念
   - 矩阵乘法和范数是最重要的运算
   - 张量运算是神经网络计算的基础
4. **微积分 (2.4)**
   - 导数衡量函数的变化率
   - 梯度是多元函数的导数向量
   - 链式法则是反向传播的理论基础
   - 梯度下降利用梯度来优化参数
5. **自动微分 (2.5)**
   - PyTorch自动计算梯度,无需手动求导
   - `requires_grad=True` + `.backward()` 是基本用法
   - 理解计算图的概念
   - 记得清零累积的梯度
6. **概率 (2.6)**
   - 概率量化不确定性
   - 条件概率和贝叶斯定理很重要
   - 正态分布是最常用的分布
   - 期望和方差衡量分布的特征
7. **查阅文档 (2.7)**
   - 善用`dir()`和`help()`
   - 熟悉PyTorch官方文档
   - 不需要记住所有函数

### 💡 学习建议

1. **不要死记硬背**
   - 这一章是工具箱,知道在哪里查即可
   - 理解概念比记忆公式重要
2. **动手实践**
   - 运行每个代码示例
   - 修改参数观察结果变化
   - 在自己的数据上尝试
3. **循序渐进**
   - 第一遍快速浏览,建立整体认识
   - 后续章节中遇到时再深入理解
   - 不懂的内容可以先跳过,不影响继续学习
4. **重点关注**
   - 张量操作 (最基础)
   - 自动微分 (最核心)
   - 其他内容了解即可

### 🎓 检验理解

能回答以下问题说明你掌握了本章:

- [ ] 什么是张量?如何创建和操作张量?
- [ ] 广播机制是什么?什么情况下会触发?
- [ ] 如何处理数据中的缺失值?
- [ ] 矩阵乘法和按元素乘法有什么区别?
- [ ] 什么是梯度?它指向什么方向?
- [ ] 如何用PyTorch自动计算梯度?
- [ ] 为什么要清零梯度?

### 📖 下一步

第三章将学习第一个真正的深度学习模型——**线性回归**,这将用到本章的所有知识!

------

## 🔗 快速参考

**张量操作**:

```python
torch.arange(n)           # 创建向量
torch.zeros(shape)        # 全0张量
torch.randn(shape)        # 随机张量
x.reshape(shape)          # 改变形状
x.sum(axis=0)            # 求和
```

**自动微分**:

```python
x = torch.tensor([1.0], requires_grad=True)
y = x ** 2
y.backward()             # 计算梯度
x.grad                   # 查看梯度
x.grad.zero_()           # 清零梯度
```

**文档查阅**:

```python
dir(torch)               # 查看所有函数
help(torch.ones)         # 查看文档
torch.ones?              # Jupyter中查看文档
```

让我为你总结第二章的内容。第二章是预备知识，包含数学基础部分。好的!现在我为你创建第二章的学习笔记。第二章是预备知识,包含数据操作、线性代数、微积分、自动求导和概率等基础内容。完成! 我为你创建了第二章预备知识的详细学习笔记。

## 📝 第二章笔记特色:

### 1️⃣ **实用导向**

- 每个概念都配有代码示例
- 强调"知道在哪查"比"记住所有"更重要
- 提供快速参考表

### 2️⃣ **结构清晰**

分7个小节:

- 2.1 数据操作(张量)
- 2.2 数据预处理
- 2.3 线性代数
- 2.4 微积分
- 2.5 自动微分 ⭐ (最重要!)
- 2.6 概率
- 2.7 查阅文档

### 3️⃣ **重点突出**

- 🎯 每节都有"重点提示"总结核心要点

- ⭐ 标注了最重要的内容(自动微分)
- 💡 提供学习建议,避免陷入细节

### 4️⃣ **图解清晰**

- 用ASCII图展示计算图、广播机制
- 用表格对比不同概念
- 用代码注释说明计算过程

## 🎯 本章核心理解:

### 最重要的3个概念:

1. **张量(Tensor)** - 数据的容器
   - 就像NumPy数组,但能在GPU上运算
   - 深度学习的所有数据都用张量表示
2. **自动微分(Autograd)** - 深度学习的魔法
   - 不需要手动求导!
   - PyTorch自动构建计算图并反向传播
   - 这是深度学习框架最核心的功能
3. **梯度下降** - 优化的原理
   - 沿着梯度的反方向更新参数
   - 微积分 + 自动微分 = 梯度下降
   - 所有训练算法的基础

## 💡 学习建议:

### 第一遍学习(快速浏览):

- ✅ 2.1 数据操作 - **必须掌握**
- ✅ 2.5 自动微分 - **必须掌握**
- ⭕ 2.3 线性代数 - 了解即可
- ⭕ 2.4 微积分 - 了解即可
- ⭕ 2.2, 2.6, 2.7 - 需要时再看

### 实践练习:

```python
# 练习1: 创建和操作张量
import torch

# 创建一个3x4的随机矩阵
X = torch.randn(3, 4)

# 计算每行的和
row_sum = X.sum(axis=1)

# 转置矩阵
X_T = X.T

print(X.shape, row_sum.shape, X_T.shape)
# 练习2: 自动求导
# 计算 y = x^2 + 2x + 1 在 x=3 时的梯度

x = torch.tensor([3.0], requires_grad=True)
y = x**2 + 2*x + 1

y.backward()

print(f"dy/dx at x=3: {x.grad}")  # 应该是 2*3+2=8
# 练习3: 数据预处理
import pandas as pd

# 创建含缺失值的数据
data = pd.DataFrame({
    'A': [1, 2, None, 4],
    'B': ['x', 'y', 'x', None],
    'C': [10, 20, 30, 40]
})

# 填充数值列的缺失值
data['A'].fillna(data['A'].mean(), inplace=True)

# 对类别列做独热编码
data_encoded = pd.get_dummies(data, columns=['B'])

# 转换为张量
tensor_data = torch.tensor(data_encoded.values, dtype=torch.float32)

print(tensor_data)
```

## 🚀 进阶提示:

### 常见误区:

1. **❌ 混淆 `\*` 和 `@`**

   ```python
   A * B   # 按元素相乘 (element-wise)
   A @ B   # 矩阵乘法 (matrix multiplication)
   ```

2. **❌ 忘记清零梯度**

   ```python
   for epoch in range(10):
       y = model(x)
       loss.backward()
       optimizer.step()
       optimizer.zero_grad()  # 必须清零!
   ```

3. **❌ 不理解广播**

   ```python
   # 形状 (3,1) + 形状 (1,4) = 形状 (3,4) ✅
   # 形状 (3,2) + 形状 (2,3) = 错误! ❌
   ```

### 调试技巧:

```python
# 1. 检查张量形状
print(f"Shape: {x.shape}")

# 2. 检查是否需要梯度
print(f"Requires grad: {x.requires_grad}")

# 3. 检查梯度是否计算
print(f"Gradient: {x.grad}")

# 4. 检查数据类型
print(f"Dtype: {x.dtype}")

# 5. 可视化数据
import matplotlib.pyplot as plt
plt.hist(x.detach().numpy(), bins=50)
plt.show()
```

## 🔗 与第一章的联系:

```
第一章: 学习了"为什么" - 深度学习的应用场景
         ↓
第二章: 学习了"用什么" - 深度学习的工具和数学基础
         ↓
第三章: 学习了"怎么做" - 第一个实际模型(线性回归)
```

## 📊 知识图谱:

```
数据操作(张量)
    ↓
线性代数(矩阵运算) ←→ 神经网络的前向传播
    ↓
微积分(导数梯度) ←→ 损失函数的优化
    ↓
自动微分(反向传播) ←→ 神经网络的训练
    ↓
概率(不确定性) ←→ 模型的预测和评估
```

## 🎓 自测题:

### 基础题:

1. 如何创建一个形状为(2,3,4)的全1张量?
2. 两个形状为(3,4)和(4,5)的矩阵相乘后形状是多少?
3. 什么是梯度?它有什么用?

### 进阶题:

1. 为什么梯度会累积?什么时候需要清零?
2. 广播机制的规则是什么?举例说明
3. 计算图在什么时候被释放?如何保留?

<details> <summary>点击查看答案</summary>

1. `torch.ones(2, 3, 4)`
2. (3, 5)
3. 梯度是函数的导数向量,指向函数增长最快的方向。在深度学习中用于更新参数以最小化损失
4. PyTorch默认累积梯度以支持某些高级用法。每次backward()后应该调用`optimizer.zero_grad()`清零
5. 从右往左比较维度,维度要么相等要么其中一个为1。例: (3,1)和(1,4)可以广播为(3,4)
6. 默认在backward()后释放。可以用`backward(retain_graph=True)`保留

</details>

------

## 你想继续学习第三章吗?

第三章是**深度学习基础**,将学习:

- 3.1 线性回归 (第一个真正的模型!)
- 3.2 从零实现线性回归
- 3.3 使用PyTorch简洁实现
- 3.4 Softmax回归(多分类)
- 3.5 图像分类实战

这一章会把第二章的所有知识都用起来,非常精彩!要继续吗? 😊